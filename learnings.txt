Credits:
    1) https://www.kaggle.com/learn/
    2) https://www.kaggle.com/kenjee/titanic-project-example/notebook

################################### Important notes ############################

Basic steps in any ML problems:
    a) Understanding the shape of the data just using plots for numerical and categorical values.
    b) Data Cleaning 
    c) Data Exploration
    d) Feature engineering
    d) Data Preprocessing
    e) Model building
    f) Model tuning
    g) Ensemble model building
    h) Results 

1) Before starting to code, always try to outline the task as a list of points and then start working on each point. We can also add interesting questions along with just correlation analysis among the input features.

2) Data Exploration: we can use the functions info(), describe() on dataframes to get a view of the input data.
    a) For numerical data: Make histograms, correlation plots(heatmaps), pivot tables.
    b) For Categorical data: Bar charts, pivot tables.

3) Split the data into numeric variables and categorical variables.
4) Plot histograms on numeric values and check whether the data is following any normal distribution.
5) correlation plots would be useful in case of regression problems.
6) Use pivot tables to check average values of different numerical features for each label.
7) Apply barplots on categorical data and observe the ratio's of categorical values. If any feature has too many distinct values then we need to simplify/reduce the data with some feature engineering techniques. 

8) If the numerical data is normally distributed then replace the missing values with the mean, otherwise with median.
9) we can use pandas get_dummies function to generate features for categorical data(similar to onehot encoding).
10) Always use k-fold cross validation for improving the model performance.
11) Identify the Target and features clearly before building the model and represent with y and X respectively.
12) Basic model validation: Calculate the Mean Absolute Error to check the error in the case of regression tasks.
13) Underfitting and Overfitting are two phenomenas/scenarios where model performs poorly on training data and test data respectively.

14) We need to optimize the parameters of the model to avoid both underfitting and overfitting.
15) While optimizing consider splitting the data into train and test datasets using train_test_split and use some model validation techniques to tune hypterparameters.
16) Once the hypterparameter values are tuned then use complete train data to train the model and use that model for prediction/submission purpose.
17) We can handle the missing values with either dropping the columns/rows or replacing/imputing with some value. Imputing value can be mean, median, min, zero ...... We should test different imputing techniques and finalize the method which is giving the lowest error.

18) Categorical values: there are mainly two types of categorical values, nominal and ordinal.
    If we can express the categories in any particular order, then it is ordinal values. If we cannot compare the values, then they are nominal values. Generally, to handle the missing values in categorical columns we replace the missing value with its mode. 

19) To handle nominal values, we can use the one-hot encoding technique. For ordinal values, we can use the label encoding technique where we assign a value to one category according to its order.

20) While performing the label encoding techniques, we should split the categorical columns into 2 types such as good_cols and bad_cols. Bad columns are those categorical columns which have different values in train and test splits. So, drop the categorical which are having different unique values in train and test splits and apply label encoding technique on good_cols. However, you can also come up with your own encoding technique which handles all values.

21) Number of unique entries in a categorical column gives the cardinality of the categorical variable. It is suggested to perform the one-hot encoding technique only when the variable has low cardinality, if the categorical variable has high cardinality, then either we should drop or use label encoding technique.

22) In both label encoding and one-hot encoding the original categorical column data needs to be dropped after the encoding.
23) Pipelining is a technique which is used to bundle Preprocessing and modelling steps so we can use the whole bundle as if it were a single step. 
24) Pipeline contains list of tuples where each tuple represents a processing steps such as Imputer, Onehotencoder ....
25) To bundle different Preprocessing steps we can use Transformers which groups many pipelines to one. 

26) Ensemble methods combine the results/predictions of several models. Ex: Randomforests which averages the predictions of many different decision trees, Gradient boosting which goes through cycles to iteratively add models into an ensemble...

27) Gradient Boosting cycle steps:
    a) Naive model
    b) make predictions
    c) calculate loss
    d) train new model
    e) add new model to the ensemble            --> and repeat from step a.

28) XGBoost - Extreme Gradient Boosting used for better performance and speed.

29) Both Randomforests and XGBoost has a common hypterparameter named n_estimators which specifies how many times to go through the modelling cycle or how many no of models we include in the ensemble.

30) Data Leakage is a scenario where some of the features in the training dataset are related to future occurance of the event/prediction. So, we should look at each column of the data and drop the columns/features which are not available at the time of training. otherwise, the model ends up with overfitting.

31) Every machine learning model contains two types of parameters. Model parameters and Hypterparameters.
    a) Model parameters: the parameters in the model that must be determined using training data set.
    b) Hypterparameters: the adjustable parameters that must be tuned in order to obtain a model with optimal performance.

32) Ensemble technique: If we combine/aggregate multiple models to find the output, then it is an ensembling technique.     
    a) They are two types of Ensemble techniques: Bagging and Boosting.
    b) Bagging is also called as Bootstrap aggregation(non-sequential).
    c) Boosting techniques are sequential

33) Bagging: RandomForest algorithm is an example of Bagging.
    - we consider multiple Decision trees on various sub-samples of the dataset
    - output of all these models are passed to voting classifiers
    - majority of these model outputs are considered as the final output.
    - The technique of using sub-sampling is similar to Row sampling with Replacement(also called as Bootstrap).
    - The final step of combining the results of individual decision trees is called as aggregater.
    
34) Boosting: Adaboost, Gradient Booting, XGBoost are different techniques of Boosting.

35) Adaboost: Adaptive Boosting combines multiple weak learners into a single strong learner.
    - This method does not follow Bootstrapping(no sub-samples, no row sampling).
    - But it will create different decision trees with a single split similar tree depth of one(shallow decision trees). These single splits are called decision stumps.
    - The number of decision stumps depends on the number of features in the dataset.
    - Similar to Bagging/Randomforests, the result is taken by majority of the decision stumps(weak learners) outputs.

36) Adaboost: At each iteration, algorithm identifies the misclassified data points, increases their weights. So that, the next classifier will pay extra attention to get them right. The weakness is estimated by the weak estimators error rate.

37) Gradient Boosting focuses on difference between the prediction and ground truth. The algorithm tries to gradually minimize the loss function(error). The learning procedure consecutively fit new models to provide a more accurate estimate of the prediction variable.

38) Boosting techniques are sequential, each estimator is built on its predecessors and at each step errors are minimized and also high performance models are generated than the last step(boosting), so hard to scale.
    
39) XGBoost is a decision tree based ensemble machine learning algorithm that uses a gradient boosting framework. XGBoost improves upon the base gradient boosting framework through systems optimization and algorithmic enhancements.

40) XGBoost features: 
    credits: https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d

        a) System optimization:
            - Parallelized implementataion of sequential tree building process.
            - Tree pruning using depth first approach.
            - Hardware optimization with cache awareness and out-of-core computing.
            
        b) Algorithmic enhancements:
            - Avoid overfitting with regularization approaches such as Lasso(L1) and Ridge(L2).
            - Efficient handling of missing data.
            - In-built cross-validation capability.

41) (Decision Tree -> Bagging -> RandomForest -> Boosting -> Adaboost -> Gradient Boosting -> XGBoost)

42) z-score normalization transforms the data into the range of distribution with mean at zero(0) and points at 1 standard deviation from the mean. 

43) To avoid one feature effect dominating on other effects, we should normalize the data with their z-scores. If possible all the continuous columns/features.

44) Target encoding for categorical values: using target for encoding purpose rather than using one-hot encoding. consider the encoding value as mean of target values by grouping with categorical feature. To overcome overfitting of this technique, we replace the least group mean values with total mean of target column.

45) 