Credits:
    1) https://www.kaggle.com/learn/
    2) https://www.kaggle.com/kenjee/titanic-project-example/notebook

################################### Important notes ############################

Basic steps in any ML problems:
    a) Understanding the shape of the data just using plots for numerical and categorical values.
    b) Data Cleaning 
    c) Data Exploration
    d) Feature engineering
    d) Data Preprocessing
    e) Model building
    f) Model tuning
    g) Ensemble model building
    h) Results 

1) Before starting to code, always try to outline the task as a list of points and then start working on each point. We can also add interesting questions along with just correlation analysis among the input features.

2) Data Exploration: we can use the functions info(), describe() on dataframes to get a view of the input data.
    a) For numerical data: Make histograms, correlation plots(heatmaps), pivot tables.
    b) For Categorical data: Bar charts, pivot tables.

3) Split the data into numeric variables and categorical variables.
4) Plot histograms on numeric values and check whether the data is following any normal distribution.
5) correlation plots would be useful in case of regression problems.
6) Use pivot tables to check average values of different numerical features for each label.
7) Apply barplots on categorical data and observe the ratio's of categorical values. If any feature has too many distinct values then we need to simplify/reduce the data with some feature engineering techniques. 

8) If the numerical data is normally distributed then replace the missing values with the mean, otherwise with median.
9) we can use pandas get_dummies function to generate features for categorical data(similar to onehot encoding).
10) Always use k-fold cross validation for improving the model performance.
11) Identify the Target and features clearly before building the model and represent with y and X respectively.
12) Basic model validation: Calculate the Mean Absolute Error to check the error in the case of regression tasks.
13) Underfitting and Overfitting are two phenomenas/scenarios where model performs poorly on training data and test data respectively.

14) We need to optimize the parameters of the model to avoid both underfitting and overfitting.
15) While optimizing consider splitting the data into train and test datasets using train_test_split and use some model validation techniques to tune hypterparameters.
16) Once the hypterparameter values are tuned then use complete train data to train the model and use that model for prediction/submission purpose.
17) We can handle the missing values with either dropping the columns/rows or replacing/imputing with some value. Imputing value can be mean, median, min, zero ...... We should test different imputing techniques and finalize the method which is giving the lowest error.

18) Categorical values: there are mainly two types of categorical values, nominal and ordinal.
    If we can express the categories in any particular order, then it is ordinal values. If we cannot compare the values, then they are nominal values. Generally, to handle the missing values in categorical columns we replace the missing value with its mode. 

19) To handle nominal values, we can use the one-hot encoding technique. For ordinal values, we can use the label encoding technique where we assign a value to one category according to its order.

20) While performing the label encoding techniques, we should split the categorical columns into 2 types such as good_cols and bad_cols. Bad columns are those categorical columns which have different values in train and test splits. So, drop the categorical which are having different unique values in train and test splits and apply label encoding technique on good_cols. However, you can also come up with your own encoding technique which handles all values.

21) Number of unique entries in a categorical column gives the cardinality of the categorical variable. It is suggested to perform the one-hot encoding technique only when the variable has low cardinality, if the categorical variable has high cardinality, then either we should drop or use label encoding technique.

22) In both label encoding and one-hot encoding the original categorical column data needs to be dropped after the encoding.
23) Pipelining is a technique which is used to bundle Preprocessing and modelling steps so we can use the whole bundle as if it were a single step. 
24) Pipeline contains list of tuples where each tuple represents a processing steps such as Imputer, Onehotencoder ....
25) To bundle different Preprocessing steps we can use Transformers which groups many pipelines to one. 

26) Ensemble methods combine the results/predictions of several models. Ex: Randomforests which averages the predictions of many different decision trees, Gradient boosting which goes through cycles to iteratively add models into an ensemble...

27) Gradient Boosting cycle steps:
    a) Naive model
    b) make predictions
    c) calculate loss
    d) train new model
    e) add new model to the ensemble            --> and repeat from step a.

28) XGBoost - Extreme Gradient Boosting used for better performance and speed.

29) Both Randomforests and XGBoost has a common hypterparameter named n_estimators which specifies how many times to go through the modelling cycle or how many no of models we include in the ensemble.

30) Data Leakage is a scenario where some of the features in the training dataset are related to future occurance of the event/prediction. So, we should look at each column of the data and drop the columns/features which are not available at the time of training. otherwise, the model ends up with overfitting.

31) 